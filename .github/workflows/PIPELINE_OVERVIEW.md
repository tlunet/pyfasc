# CI/CD Pipeline Ãœbersicht

## ğŸ”„ Pipeline Workflow

```mermaid
graph TD
    A[Code Change im src/ Ordner] --> B{Push oder PR?}
    B -->|Push| C[Trigger auf main/develop]
    B -->|Pull Request| D[Trigger auf main]
    
    C --> E[Checkout Repository]
    D --> E
    
    E --> F[Git Diff: Erkenne geÃ¤nderte Dateien]
    
    F --> G{Welche Dateitypen?}
    
    G -->|.py| H[Python Setup]
    G -->|.cpp| I[C++ Setup + Compiler]
    G -->|.jl| J[Julia Setup]
    
    H --> K[Install numpy, scipy]
    I --> L[Install g++]
    J --> M[Install Julia 1.9]
    
    K --> N[Grid Convergence Test - Python]
    L --> O[Grid Convergence Test - C++]
    M --> P[Grid Convergence Test - Julia]
    
    N --> Q{Test bestanden?}
    O --> Q
    P --> Q
    
    Q -->|Ja| R[âœ… SUCCESS - Merge erlaubt]
    Q -->|Nein| S[âŒ FAILURE - Merge blockiert]
    
    R --> T[Deployment mÃ¶glich]
    S --> U[Code Review & Fix erforderlich]
```

## ğŸ“Š Test-Ablauf im Detail

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant GH as GitHub
    participant CI as CI Pipeline
    participant Test as Grid Convergence Test
    
    Dev->>GH: Push neuen Code zu src/program.py
    GH->>CI: Trigger Workflow
    CI->>CI: Checkout Code
    CI->>CI: Erkenne: program.py geÃ¤ndert
    CI->>CI: Setup Python 3.10
    CI->>CI: Install numpy, scipy
    
    CI->>Test: Starte Test mit Grid 8x8
    Test-->>CI: LÃ¶sung berechnet âœ“
    
    CI->>Test: Starte Test mit Grid 16x16
    Test-->>CI: LÃ¶sung berechnet âœ“
    
    CI->>Test: Starte Test mit Grid 32x32
    Test-->>CI: LÃ¶sung berechnet âœ“
    
    CI->>Test: Starte Test mit Grid 64x64
    Test-->>CI: LÃ¶sung berechnet âœ“
    
    CI->>Test: Berechne L2-Fehler
    Test-->>CI: Fehler: [0.089, 0.037, 0.014]
    
    CI->>Test: Berechne Konvergenzordnung
    Test-->>CI: Ordnung: 1.33 Â± 0.06
    
    CI->>Test: Validiere Ordnung (1.0 < p < 4.0)
    Test-->>CI: âœ… BESTANDEN
    
    CI-->>GH: Test erfolgreich âœ…
    GH-->>Dev: Status: All checks passed
```

## ğŸ¯ Automatische Validierung pro Datei

```
src/
â”œâ”€â”€ program.py      â†’ Automatisch getestet bei Ã„nderung
â”œâ”€â”€ program.cpp     â†’ Automatisch getestet bei Ã„nderung
â”œâ”€â”€ program.jl      â†’ Automatisch getestet bei Ã„nderung
â”œâ”€â”€ solver_v2.py    â†’ Automatisch getestet bei Ã„nderung
â””â”€â”€ my_algo.cpp     â†’ Automatisch getestet bei Ã„nderung
```

**Jede neue oder geÃ¤nderte Datei im `src/` Ordner wird automatisch validiert!**

## âš™ï¸ Konfigurierbare Parameter

In `run_validation.py` und den Helper-Scripts:

```python
# Test-GittergrÃ¶ÃŸen (in run_*_multi_grids.py)
grid_sizes = [256, 128, 64]

# Test-Zeitschritte (in run_*_multi_steps.py)
time_steps = [16, 32, 64]

# Toleranzen fÃ¼r Vergleiche (in run_validation.py)
rtol = 1e-6  # Relative Toleranz
atol = 1e-6  # Absolute Toleranz
```

## ğŸš¦ Status-Badges

FÃ¼gen Sie in Ihre README.md ein:

```markdown
![Grid Convergence](https://github.com/tlunet/pyfasc/workflows/Grid%20Convergence%20Validation/badge.svg)
```

## ğŸ“ˆ Metriken

Die Pipeline trackt:
- âœ… Anzahl erfolgreich getesteter Dateien
- âŒ Anzahl fehlgeschlagener Tests
- ğŸ“Š Durchschnittliche Konvergenzordnung
- â±ï¸ AusfÃ¼hrungszeit pro Test
